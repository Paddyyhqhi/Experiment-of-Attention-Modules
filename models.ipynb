{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497072cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import resnet\n",
    "import torch\n",
    "from torchvision import models\n",
    "from torchvision.models import ResNet34_Weights, ResNet50_Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0442414c",
   "metadata": {},
   "source": [
    "## DANet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9a9cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PAM_Module(nn.Module):\n",
    "    def __init__(self, in_dim):\n",
    "        super(PAM_Module, self).__init__()\n",
    "        self.channel_in = in_dim\n",
    "        self.query_conv = nn.Conv2d(in_dim, in_dim // 8, kernel_size=1, stride=1, padding='valid')\n",
    "        self.key_conv = nn.Conv2d(in_dim, in_dim // 8, kernel_size=1, stride=1, padding='valid')\n",
    "        self.value_conv = nn.Conv2d(in_dim, in_dim, kernel_size=1, stride=1, padding='valid')\n",
    "        self.gamma = nn.Parameter(torch.zeros(1))\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        m_batchsize, C, height, width = x.size()\n",
    "        proj_query = self.query_conv(x).view(m_batchsize, -1, width * height).permute(0, 2, 1)\n",
    "        proj_key = self.key_conv(x).view(m_batchsize, -1, width * height)\n",
    "        energy = torch.bmm(proj_query, proj_key)\n",
    "        attention = self.softmax(energy)\n",
    "        proj_value = self.value_conv(x).view(m_batchsize, -1, width * height)\n",
    "        proj_value = proj_value.permute(0, 2, 1)\n",
    "        out = torch.bmm(attention, proj_value).view(m_batchsize, C, height, width)\n",
    "        out = self.gamma * out + x\n",
    "        return out\n",
    "\n",
    "class CAM_Module(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CAM_Module, self).__init__()\n",
    "        self.gamma = nn.Parameter(torch.zeros(1))\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        m_batchsize, C, height, width = x.size()\n",
    "        proj_query = x.view(m_batchsize, C, -1)\n",
    "        proj_key = x.view(m_batchsize, C, -1).permute(0, 2, 1)\n",
    "        energy = torch.bmm(proj_query, proj_key)\n",
    "        energy_new = energy.max(dim=-1, keepdim=True)[0]\n",
    "        energy_new = energy_new - energy\n",
    "        attention = self.softmax(energy_new)\n",
    "        proj_value = x.view(m_batchsize, C, -1)\n",
    "\n",
    "        out = torch.bmm(attention, proj_value).view(m_batchsize, C, height, width)\n",
    "        out = self.gamma * out + x\n",
    "        return out\n",
    "\n",
    "class DANet(nn.Module):\n",
    "    def __init__(self, in_channels=512, out_channels=128, out_dim=8):\n",
    "        super(DANet, self).__init__()\n",
    "        inter_channels = in_channels // 4\n",
    "        base_model = models.resnet34(weights=ResNet34_Weights.IMAGENET1K_V1)\n",
    "        self.backbone = torch.nn.Sequential(*list(base_model.children())[:-2])\n",
    "\n",
    "        self.conv5a = nn.Conv2d(in_channels, inter_channels, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(inter_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv5c = nn.Conv2d(in_channels, inter_channels, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(inter_channels)\n",
    "        self.sa = PAM_Module(inter_channels)\n",
    "        self.sc = CAM_Module()\n",
    "        self.conv51 = nn.Conv2d(inter_channels, inter_channels, kernel_size=3, padding=1)\n",
    "        self.conv52 = nn.Conv2d(inter_channels, inter_channels, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(inter_channels)\n",
    "        self.bn4 = nn.BatchNorm2d(inter_channels)\n",
    "        self.dropout = nn.Dropout2d(0.1)\n",
    "        self.conv6 = nn.Conv2d(inter_channels, out_channels, kernel_size=1)\n",
    "        self.conv7 = nn.Conv2d(inter_channels, out_channels, kernel_size=1)\n",
    "        self.conv8 = nn.Conv2d(inter_channels, out_channels, kernel_size=1)\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(6272, 2048),\n",
    "            nn.BatchNorm1d(2048),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(2048, out_dim)\n",
    "        )\n",
    "\n",
    "        self.feat = None\n",
    "        self.gradients = None\n",
    "    def activations_hook(self, grad):\n",
    "        self.gradients = grad\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        feat1 = self.relu(self.bn1(self.conv5a(x)))\n",
    "        sa_feat = self.sa(feat1)\n",
    "        sa_conv = self.relu(self.bn3(self.conv51(sa_feat)))\n",
    "        sa_output = self.conv6(self.dropout(sa_conv))\n",
    "\n",
    "        feat2 = self.relu(self.bn2(self.conv5c(x)))\n",
    "        sc_feat = self.sc(feat2)\n",
    "        sc_conv = self.relu(self.bn4(self.conv52(sc_feat)))\n",
    "        sc_output = self.conv7(self.dropout(sc_conv))\n",
    "\n",
    "        # feat_sum = sa_conv + sc_conv\n",
    "        feat_sum = sa_output + sc_output\n",
    "        sasc_output = self.conv8(self.dropout(feat_sum))\n",
    "        self.feat = sasc_output\n",
    "        h = sasc_output.register_hook(self.activations_hook)\n",
    "        output = nn.Flatten()(sasc_output)\n",
    "        output = self.fc(output)\n",
    "        return output\n",
    "    \n",
    "    def get_activations_gradient(self):\n",
    "        return self.gradients\n",
    "    \n",
    "    def get_activations(self):\n",
    "        return self.feat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b87859",
   "metadata": {},
   "source": [
    "## CBAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab7ada1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    \"3x3 convolution with padding\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=1, bias=False)\n",
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, in_planes, ratio=16):\n",
    "        super(ChannelAttention, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "           \n",
    "        self.fc = nn.Sequential(nn.Conv2d(in_planes, in_planes // 16, 1, bias=False),\n",
    "                               nn.ReLU(),\n",
    "                               nn.Conv2d(in_planes // 16, in_planes, 1, bias=False))\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = self.fc(self.avg_pool(x))\n",
    "        max_out = self.fc(self.max_pool(x))\n",
    "        out = avg_out + max_out\n",
    "        return self.sigmoid(out)\n",
    "\n",
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self, kernel_size=7):\n",
    "        super(SpatialAttention, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(2, 1, kernel_size, padding=kernel_size//2, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        x = torch.cat([avg_out, max_out], dim=1)\n",
    "        x = self.conv1(x)\n",
    "        return self.sigmoid(x)\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.ca = ChannelAttention(planes)\n",
    "        self.sa = SpatialAttention()\n",
    "\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        out = self.ca(out) * out\n",
    "        out = self.sa(out) * out\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out    \n",
    "\n",
    "class CBAM(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes=8):\n",
    "        self.inplanes = 512\n",
    "        super(CBAM, self).__init__()\n",
    "        base_model = models.resnet34(weights=ResNet34_Weights.IMAGENET1K_V1)\n",
    "        self.backbone = torch.nn.Sequential(*list(base_model.children())[:-2])\n",
    "        self.layer = self._make_layer(BasicBlock, 512, 2, stride=2)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512, out_dim)\n",
    "        self.feat = None\n",
    "        self.gradients = None\n",
    "        \n",
    "    def activations_hook(self, grad):\n",
    "        self.gradients = grad\n",
    "        \n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = self.layer(x)\n",
    "        self.feat = x\n",
    "        h = x.register_hook(self.activations_hook)\n",
    "        output = self.avgpool(x)\n",
    "        output = output.view(output.size(0), -1)\n",
    "        output = self.fc(output)\n",
    "\n",
    "        return output\n",
    "    \n",
    "    def get_activations_gradient(self):\n",
    "        return self.gradients\n",
    "    \n",
    "    def get_activations(self):\n",
    "        return self.feat\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2201740",
   "metadata": {},
   "source": [
    "## PAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae89897",
   "metadata": {},
   "outputs": [],
   "source": [
    "import resnet\n",
    "\n",
    "class ResNet50(nn.Module):\n",
    "    def __init__(self, pretrained=True):\n",
    "        \"\"\"Declare all needed layers.\"\"\"\n",
    "        super(ResNet50, self).__init__()\n",
    "        self.model = resnet.resnet50(pretrained=pretrained)\n",
    "        self.relu = self.model.relu  # Place a hook\n",
    "\n",
    "        layers_cfg = [4, 5, 6, 7]\n",
    "        self.blocks = []\n",
    "        for i, num_this_layer in enumerate(layers_cfg):\n",
    "            self.blocks.append(list(self.model.children())[num_this_layer])\n",
    "\n",
    "    def forward(self, x):\n",
    "        feature_map = []\n",
    "        x = self.model.conv1(x)\n",
    "        x = self.model.bn1(x)\n",
    "        x = self.model.relu(x)\n",
    "        x = self.model.maxpool(x)\n",
    "\n",
    "        for i, block in enumerate(self.blocks):\n",
    "            x = block(x)\n",
    "            feature_map.append(x)\n",
    "\n",
    "        out = nn.AvgPool2d(x.shape[2:])(x).view(x.shape[0], -1)\n",
    "\n",
    "        return feature_map, out\n",
    "    \n",
    "class ResNet34(nn.Module):\n",
    "    def __init__(self, pretrained=True):\n",
    "        \"\"\"Declare all needed layers.\"\"\"\n",
    "        super(ResNet34, self).__init__()\n",
    "        self.model = resnet.resnet34(pretrained=pretrained)\n",
    "        self.relu = self.model.relu  # Place a hook\n",
    "\n",
    "        layers_cfg = [4, 5, 6, 7]\n",
    "        self.blocks = []\n",
    "        for i, num_this_layer in enumerate(layers_cfg):\n",
    "            self.blocks.append(list(self.model.children())[num_this_layer])\n",
    "\n",
    "    def forward(self, x):\n",
    "        feature_map = []\n",
    "        x = self.model.conv1(x)\n",
    "        x = self.model.bn1(x)\n",
    "        x = self.model.relu(x)\n",
    "        x = self.model.maxpool(x)\n",
    "\n",
    "        for i, block in enumerate(self.blocks):\n",
    "            x = block(x)\n",
    "            feature_map.append(x)\n",
    "\n",
    "        out = nn.AvgPool2d(x.shape[2:])(x).view(x.shape[0], -1)\n",
    "\n",
    "        return feature_map, out\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, in_features=2048, num_class=20):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_features, num_class)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "\n",
    "class FPA(nn.Module):\n",
    "    def __init__(self, channels=2048):\n",
    "        \"\"\"\n",
    "        Feature Pyramid Attention\n",
    "        :type channels: int\n",
    "        \"\"\"\n",
    "        super(FPA, self).__init__()\n",
    "        channels_mid = int(channels/4)\n",
    "\n",
    "        self.channels_cond = channels\n",
    "\n",
    "        # Master branch\n",
    "        self.conv_master = nn.Conv2d(self.channels_cond, channels, kernel_size=1, bias=False)\n",
    "        self.bn_master = nn.BatchNorm2d(channels)\n",
    "\n",
    "        # Global pooling branch\n",
    "        self.conv_gpb = nn.Conv2d(self.channels_cond, channels, kernel_size=1, bias=False)\n",
    "        self.bn_gpb = nn.BatchNorm2d(channels)\n",
    "\n",
    "        # C333 because of the shape of last feature maps is (16, 16).\n",
    "        self.conv7x7_1 = nn.Conv2d(self.channels_cond, channels_mid, kernel_size=(7, 7), stride=2, padding=3, bias=False)\n",
    "        self.bn1_1 = nn.BatchNorm2d(channels_mid)\n",
    "        self.conv5x5_1 = nn.Conv2d(channels_mid, channels_mid, kernel_size=(5, 5), stride=2, padding=2, bias=False)\n",
    "        self.bn2_1 = nn.BatchNorm2d(channels_mid)\n",
    "        self.conv3x3_1 = nn.Conv2d(channels_mid, channels_mid, kernel_size=(3, 3), stride=2, padding=1, bias=False)\n",
    "        self.bn3_1 = nn.BatchNorm2d(channels_mid)\n",
    "\n",
    "        self.conv7x7_2 = nn.Conv2d(channels_mid, channels_mid, kernel_size=(7, 7), stride=1, padding=3, bias=False)\n",
    "        self.bn1_2 = nn.BatchNorm2d(channels_mid)\n",
    "        self.conv5x5_2 = nn.Conv2d(channels_mid, channels_mid, kernel_size=(5, 5), stride=1, padding=2, bias=False)\n",
    "        self.bn2_2 = nn.BatchNorm2d(channels_mid)\n",
    "        self.conv3x3_2 = nn.Conv2d(channels_mid, channels_mid, kernel_size=(3, 3), stride=1, padding=1, bias=False)\n",
    "        self.bn3_2 = nn.BatchNorm2d(channels_mid)\n",
    "\n",
    "        # Convolution Upsample\n",
    "        self.conv_upsample_3 = nn.ConvTranspose2d(channels_mid, channels_mid, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "        self.bn_upsample_3 = nn.BatchNorm2d(channels_mid)\n",
    "\n",
    "        self.conv_upsample_2 = nn.ConvTranspose2d(channels_mid, channels_mid, kernel_size=4, stride=1, padding=0, bias=False)\n",
    "        self.bn_upsample_2 = nn.BatchNorm2d(channels_mid)\n",
    "\n",
    "        self.conv_upsample_1 = nn.ConvTranspose2d(channels_mid, channels, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "        self.bn_upsample_1 = nn.BatchNorm2d(channels)\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: Shape: [b, 2048, h, w]\n",
    "        :return: out: Feature maps. Shape: [b, 2048, h, w]\n",
    "        \"\"\"\n",
    "        # Master branch\n",
    "        x_master = self.conv_master(x)\n",
    "        x_master = self.bn_master(x_master)\n",
    "\n",
    "        # Global pooling branch\n",
    "        x_gpb = nn.AvgPool2d(x.shape[2:])(x).view(x.shape[0], self.channels_cond, 1, 1)\n",
    "        x_gpb = self.conv_gpb(x_gpb)\n",
    "        x_gpb = self.bn_gpb(x_gpb)\n",
    "\n",
    "        # Branch 1\n",
    "        x1_1 = self.conv7x7_1(x)\n",
    "        x1_1 = self.bn1_1(x1_1)\n",
    "        x1_1 = self.relu(x1_1)\n",
    "        x1_2 = self.conv7x7_2(x1_1)\n",
    "        x1_2 = self.bn1_2(x1_2)\n",
    "\n",
    "        # Branch 2\n",
    "        x2_1 = self.conv5x5_1(x1_1)\n",
    "        x2_1 = self.bn2_1(x2_1)\n",
    "        x2_1 = self.relu(x2_1)\n",
    "        x2_2 = self.conv5x5_2(x2_1)\n",
    "        x2_2 = self.bn2_2(x2_2)\n",
    "\n",
    "        # Branch 3\n",
    "        x3_1 = self.conv3x3_1(x2_1)\n",
    "        x3_1 = self.bn3_1(x3_1)\n",
    "        x3_1 = self.relu(x3_1)\n",
    "        x3_2 = self.conv3x3_2(x3_1)\n",
    "        x3_2 = self.bn3_2(x3_2)\n",
    "\n",
    "        # Merge branch 1 and 2\n",
    "        x3_upsample = self.relu(self.bn_upsample_3(self.conv_upsample_3(x3_2)))\n",
    "        x2_merge = self.relu(x2_2 + x3_upsample)\n",
    "        x2_upsample = self.relu(self.bn_upsample_2(self.conv_upsample_2(x2_merge)))\n",
    "        x1_merge = self.relu(x1_2 + x2_upsample)\n",
    "\n",
    "        x_master = x_master * self.relu(self.bn_upsample_1(self.conv_upsample_1(x1_merge)))\n",
    "\n",
    "        #\n",
    "        out = self.relu(x_master + x_gpb)\n",
    "\n",
    "        return out\n",
    "\n",
    "class GAU(nn.Module):\n",
    "    def __init__(self, channels_high, channels_low, upsample=True):\n",
    "        super(GAU, self).__init__()\n",
    "        # Global Attention Upsample\n",
    "        self.upsample = upsample\n",
    "        self.conv3x3 = nn.Conv2d(channels_low, channels_low, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn_low = nn.BatchNorm2d(channels_low)\n",
    "\n",
    "        self.conv1x1 = nn.Conv2d(channels_high, channels_low, kernel_size=1, padding=0, bias=False)\n",
    "        self.bn_high = nn.BatchNorm2d(channels_low)\n",
    "\n",
    "        if upsample:\n",
    "            self.conv_upsample = nn.ConvTranspose2d(channels_high, channels_low, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "            self.bn_upsample = nn.BatchNorm2d(channels_low)\n",
    "        else:\n",
    "            self.conv_reduction = nn.Conv2d(channels_high, channels_low, kernel_size=1, padding=0, bias=False)\n",
    "            self.bn_reduction = nn.BatchNorm2d(channels_low)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, fms_high, fms_low, fm_mask=None):\n",
    "        \"\"\"\n",
    "        Use the high level features with abundant catagory information to weight the low level features with pixel\n",
    "        localization information. In the meantime, we further use mask feature maps with catagory-specific information\n",
    "        to localize the mask position.\n",
    "        :param fms_high: Features of high level. Tensor.\n",
    "        :param fms_low: Features of low level.  Tensor.\n",
    "        :param fm_mask:\n",
    "        :return: fms_att_upsample\n",
    "        \"\"\"\n",
    "        b, c, h, w = fms_high.shape\n",
    "\n",
    "        fms_high_gp = nn.AvgPool2d(fms_high.shape[2:])(fms_high).view(len(fms_high), c, 1, 1)\n",
    "        fms_high_gp = self.conv1x1(fms_high_gp)\n",
    "        fms_high_gp = self.bn_high(fms_high_gp)\n",
    "        fms_high_gp = self.relu(fms_high_gp)\n",
    "\n",
    "        # fms_low_mask = torch.cat([fms_low, fm_mask], dim=1)\n",
    "        fms_low_mask = self.conv3x3(fms_low)\n",
    "        fms_low_mask = self.bn_low(fms_low_mask)\n",
    "\n",
    "        fms_att = fms_low_mask * fms_high_gp\n",
    "        if self.upsample:\n",
    "            out = self.relu(\n",
    "                self.bn_upsample(self.conv_upsample(fms_high)) + fms_att)\n",
    "        else:\n",
    "            out = self.relu(\n",
    "                self.bn_reduction(self.conv_reduction(fms_high)) + fms_att)\n",
    "\n",
    "        return out\n",
    "    \n",
    "    \n",
    "class PAN(nn.Module):\n",
    "    def __init__(self, blocks=[]):\n",
    "        \"\"\"\n",
    "        :param blocks: Blocks of the network with reverse sequential.\n",
    "        \"\"\"\n",
    "        super(PAN, self).__init__()\n",
    "        channels_blocks = []\n",
    "        for i, block in enumerate(blocks):\n",
    "            channels_blocks.append(list(list(block.children())[2].children())[4].weight.shape[0])\n",
    "\n",
    "        self.fpa = FPA(channels=channels_blocks[0])\n",
    "        # channels_high = channels_blocks[0]\n",
    "        # for i, channels_low in enumerate(channels_blocks[1:]):\n",
    "        #     self.gau.append(GAU(channels_high, channels_low))\n",
    "        #     channels_high = channels_low\n",
    "        self.gau_block1 = GAU(channels_blocks[0], channels_blocks[1], upsample=False)\n",
    "        self.gau_block2 = GAU(channels_blocks[1], channels_blocks[2])\n",
    "        self.gau_block3 = GAU(channels_blocks[2], channels_blocks[3])\n",
    "        self.gau = [self.gau_block1, self.gau_block2, self.gau_block3]\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, fms=[]):\n",
    "        \"\"\"\n",
    "        :param fms: Feature maps of forward propagation in the network with reverse sequential. shape:[b, c, h, w]\n",
    "        :return: fm_high. [b, 256, h, w]\n",
    "        \"\"\"\n",
    "        for i, fm_low in enumerate(fms):\n",
    "            if i == 0:\n",
    "                fm_high = self.fpa(fm_low)\n",
    "            else:\n",
    "                fm_high = self.gau[int(i-1)](fm_high, fm_low)\n",
    "\n",
    "        return fm_high\n",
    "\n",
    "class PAN_final(nn.Module):\n",
    "    def __init__(self, num_class=8):\n",
    "        super(PAN_final, self).__init__()\n",
    "        \n",
    "        self.convnet = ResNet34(pretrained=True)\n",
    "        self.pan = PAN(self.convnet.blocks[::-1])\n",
    "        self.cls = Classifier(in_features=64, num_class=num_class)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.feat = None\n",
    "        self.gradients = None\n",
    "        \n",
    "    def activations_hook(self, grad):\n",
    "        self.gradients = grad\n",
    "        \n",
    "    def forward(self, x):\n",
    "        fms_blob, _ = self.convnet(x)\n",
    "        out_ss = self.pan(fms_blob[::-1])\n",
    "        self.feat = out_ss\n",
    "        h = out_ss.register_hook(self.activations_hook)\n",
    "        out = self.avgpool(out_ss)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.cls(out)\n",
    "        return out\n",
    "    \n",
    "    def get_activations_gradient(self):\n",
    "        return self.gradients\n",
    "    \n",
    "    def get_activations(self):\n",
    "        return self.feat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20efe6f",
   "metadata": {},
   "source": [
    "## CBAM_DA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5248b1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    \"3x3 convolution with padding\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=1, bias=False)\n",
    "\n",
    "class PAM_Module(nn.Module):\n",
    "    def __init__(self, in_dim):\n",
    "        super(PAM_Module, self).__init__()\n",
    "        self.channel_in = in_dim\n",
    "        self.query_conv = nn.Conv2d(in_dim, in_dim // 4, kernel_size=1, stride=1, padding='valid')\n",
    "        self.key_conv = nn.Conv2d(in_dim, in_dim // 4, kernel_size=1, stride=1, padding='valid')\n",
    "        self.value_conv = nn.Conv2d(in_dim, in_dim, kernel_size=1, stride=1, padding='valid')\n",
    "        self.gamma = nn.Parameter(torch.zeros(1))\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        m_batchsize, C, height, width = x.size()\n",
    "        proj_query = self.query_conv(x).view(m_batchsize, -1, width * height).permute(0, 2, 1)\n",
    "        proj_key = self.key_conv(x).view(m_batchsize, -1, width * height)\n",
    "        energy = torch.bmm(proj_query, proj_key)\n",
    "        attention = self.softmax(energy)\n",
    "        proj_value = self.value_conv(x).view(m_batchsize, -1, width * height)\n",
    "        proj_value = proj_value.permute(0, 2, 1)\n",
    "        out = torch.bmm(attention, proj_value).view(m_batchsize, C, height, width)\n",
    "        out = self.gamma * out + x\n",
    "        return out\n",
    "\n",
    "class CAM_Module(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CAM_Module, self).__init__()\n",
    "        self.gamma = nn.Parameter(torch.zeros(1))\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        m_batchsize, C, height, width = x.size()\n",
    "        proj_query = x.view(m_batchsize, C, -1)\n",
    "        proj_key = x.view(m_batchsize, C, -1).permute(0, 2, 1)\n",
    "        energy = torch.bmm(proj_query, proj_key)\n",
    "        energy_new = energy.max(dim=-1, keepdim=True)[0]\n",
    "        energy_new = energy_new - energy\n",
    "        attention = self.softmax(energy_new)\n",
    "        proj_value = x.view(m_batchsize, C, -1)\n",
    "\n",
    "        out = torch.bmm(attention, proj_value).view(m_batchsize, C, height, width)\n",
    "        out = self.gamma * out + x\n",
    "        return out\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.ca = PAM_Module(planes)\n",
    "        self.sa = CAM_Module()\n",
    "\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        out = self.ca(out)\n",
    "        out = self.sa(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class CBAM_DA(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes=8):\n",
    "        self.inplanes = 512\n",
    "        super(CBAM_DA, self).__init__()\n",
    "        base_model = models.resnet34(weights=ResNet34_Weights.IMAGENET1K_V1)\n",
    "        self.backbone = torch.nn.Sequential(*list(base_model.children())[:-2])\n",
    "        self.layer = self._make_layer(BasicBlock, 512, 2, stride=2)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "        self.feat = None\n",
    "        self.gradients = None\n",
    "        \n",
    "    def activations_hook(self, grad):\n",
    "        self.gradients = grad\n",
    "        \n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = self.layer(x)\n",
    "        self.feat = x\n",
    "        h = x.register_hook(self.activations_hook)\n",
    "        output = self.avgpool(x)\n",
    "        output = output.view(output.size(0), -1)\n",
    "        output = self.fc(output)\n",
    "\n",
    "        return output\n",
    "    \n",
    "    def get_activations_gradient(self):\n",
    "        return self.gradients\n",
    "    \n",
    "    def get_activations(self):\n",
    "        return self.feat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684f7037",
   "metadata": {},
   "source": [
    "# PAN_DA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e7f965",
   "metadata": {},
   "outputs": [],
   "source": [
    "import resnet\n",
    "\n",
    "class ResNet50(nn.Module):\n",
    "    def __init__(self, pretrained=True):\n",
    "        \"\"\"Declare all needed layers.\"\"\"\n",
    "        super(ResNet50, self).__init__()\n",
    "        self.model = resnet.resnet50(pretrained=pretrained)\n",
    "        self.relu = self.model.relu  # Place a hook\n",
    "\n",
    "        layers_cfg = [4, 5, 6, 7]\n",
    "        self.blocks = []\n",
    "        for i, num_this_layer in enumerate(layers_cfg):\n",
    "            self.blocks.append(list(self.model.children())[num_this_layer])\n",
    "\n",
    "    def forward(self, x):\n",
    "        feature_map = []\n",
    "        x = self.model.conv1(x)\n",
    "        x = self.model.bn1(x)\n",
    "        x = self.model.relu(x)\n",
    "        x = self.model.maxpool(x)\n",
    "\n",
    "        for i, block in enumerate(self.blocks):\n",
    "            x = block(x)\n",
    "            feature_map.append(x)\n",
    "\n",
    "        out = nn.AvgPool2d(x.shape[2:])(x).view(x.shape[0], -1)\n",
    "\n",
    "        return feature_map, out\n",
    "    \n",
    "class ResNet34(nn.Module):\n",
    "    def __init__(self, pretrained=True):\n",
    "        \"\"\"Declare all needed layers.\"\"\"\n",
    "        super(ResNet34, self).__init__()\n",
    "        self.model = resnet.resnet34(pretrained=pretrained)\n",
    "        self.relu = self.model.relu  # Place a hook\n",
    "\n",
    "        layers_cfg = [4, 5, 6, 7]\n",
    "        self.blocks = []\n",
    "        for i, num_this_layer in enumerate(layers_cfg):\n",
    "            self.blocks.append(list(self.model.children())[num_this_layer])\n",
    "\n",
    "    def forward(self, x):\n",
    "        feature_map = []\n",
    "        x = self.model.conv1(x)\n",
    "        x = self.model.bn1(x)\n",
    "        x = self.model.relu(x)\n",
    "        x = self.model.maxpool(x)\n",
    "\n",
    "        for i, block in enumerate(self.blocks):\n",
    "            x = block(x)\n",
    "            feature_map.append(x)\n",
    "\n",
    "        out = nn.AvgPool2d(x.shape[2:])(x).view(x.shape[0], -1)\n",
    "\n",
    "        return feature_map, out\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, in_features=2048, num_class=20):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_features, num_class)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "\n",
    "class FPA(nn.Module):\n",
    "    def __init__(self, channels=2048):\n",
    "        \"\"\"\n",
    "        Feature Pyramid Attention\n",
    "        :type channels: int\n",
    "        \"\"\"\n",
    "        super(FPA, self).__init__()\n",
    "        channels_mid = int(channels/4)\n",
    "\n",
    "        self.channels_cond = channels\n",
    "\n",
    "        # Master branch\n",
    "        self.conv_master = nn.Conv2d(self.channels_cond, channels, kernel_size=1, bias=False)\n",
    "        self.bn_master = nn.BatchNorm2d(channels)\n",
    "\n",
    "        # Global pooling branch\n",
    "        self.conv_gpb = nn.Conv2d(self.channels_cond, channels, kernel_size=1, bias=False)\n",
    "        self.bn_gpb = nn.BatchNorm2d(channels)\n",
    "\n",
    "        # C333 because of the shape of last feature maps is (16, 16).\n",
    "        self.conv7x7_1 = nn.Conv2d(self.channels_cond, channels_mid, kernel_size=(7, 7), stride=2, padding=3, bias=False)\n",
    "        self.bn1_1 = nn.BatchNorm2d(channels_mid)\n",
    "        self.conv5x5_1 = nn.Conv2d(channels_mid, channels_mid, kernel_size=(5, 5), stride=2, padding=2, bias=False)\n",
    "        self.bn2_1 = nn.BatchNorm2d(channels_mid)\n",
    "        self.conv3x3_1 = nn.Conv2d(channels_mid, channels_mid, kernel_size=(3, 3), stride=2, padding=1, bias=False)\n",
    "        self.bn3_1 = nn.BatchNorm2d(channels_mid)\n",
    "\n",
    "        self.conv7x7_2 = nn.Conv2d(channels_mid, channels_mid, kernel_size=(7, 7), stride=1, padding=3, bias=False)\n",
    "        self.bn1_2 = nn.BatchNorm2d(channels_mid)\n",
    "        self.conv5x5_2 = nn.Conv2d(channels_mid, channels_mid, kernel_size=(5, 5), stride=1, padding=2, bias=False)\n",
    "        self.bn2_2 = nn.BatchNorm2d(channels_mid)\n",
    "        self.conv3x3_2 = nn.Conv2d(channels_mid, channels_mid, kernel_size=(3, 3), stride=1, padding=1, bias=False)\n",
    "        self.bn3_2 = nn.BatchNorm2d(channels_mid)\n",
    "\n",
    "        # Convolution Upsample\n",
    "        self.conv_upsample_3 = nn.ConvTranspose2d(channels_mid, channels_mid, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "        self.bn_upsample_3 = nn.BatchNorm2d(channels_mid)\n",
    "\n",
    "        self.conv_upsample_2 = nn.ConvTranspose2d(channels_mid, channels_mid, kernel_size=4, stride=1, padding=0, bias=False)\n",
    "        self.bn_upsample_2 = nn.BatchNorm2d(channels_mid)\n",
    "\n",
    "        self.conv_upsample_1 = nn.ConvTranspose2d(channels_mid, channels, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "        self.bn_upsample_1 = nn.BatchNorm2d(channels)\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: Shape: [b, 2048, h, w]\n",
    "        :return: out: Feature maps. Shape: [b, 2048, h, w]\n",
    "        \"\"\"\n",
    "        # Master branch\n",
    "        x_master = self.conv_master(x)\n",
    "        x_master = self.bn_master(x_master)\n",
    "\n",
    "        # Global pooling branch\n",
    "        x_gpb = nn.AvgPool2d(x.shape[2:])(x).view(x.shape[0], self.channels_cond, 1, 1)\n",
    "        x_gpb = self.conv_gpb(x_gpb)\n",
    "        x_gpb = self.bn_gpb(x_gpb)\n",
    "\n",
    "        # Branch 1\n",
    "        x1_1 = self.conv7x7_1(x)\n",
    "        x1_1 = self.bn1_1(x1_1)\n",
    "        x1_1 = self.relu(x1_1)\n",
    "        x1_2 = self.conv7x7_2(x1_1)\n",
    "        x1_2 = self.bn1_2(x1_2)\n",
    "\n",
    "        # Branch 2\n",
    "        x2_1 = self.conv5x5_1(x1_1)\n",
    "        x2_1 = self.bn2_1(x2_1)\n",
    "        x2_1 = self.relu(x2_1)\n",
    "        x2_2 = self.conv5x5_2(x2_1)\n",
    "        x2_2 = self.bn2_2(x2_2)\n",
    "\n",
    "        # Branch 3\n",
    "        x3_1 = self.conv3x3_1(x2_1)\n",
    "        x3_1 = self.bn3_1(x3_1)\n",
    "        x3_1 = self.relu(x3_1)\n",
    "        x3_2 = self.conv3x3_2(x3_1)\n",
    "        x3_2 = self.bn3_2(x3_2)\n",
    "\n",
    "        # Merge branch 1 and 2\n",
    "        x3_upsample = self.relu(self.bn_upsample_3(self.conv_upsample_3(x3_2)))\n",
    "        x2_merge = self.relu(x2_2 + x3_upsample)\n",
    "        x2_upsample = self.relu(self.bn_upsample_2(self.conv_upsample_2(x2_merge)))\n",
    "        x1_merge = self.relu(x1_2 + x2_upsample)\n",
    "\n",
    "        x_master = x_master * self.relu(self.bn_upsample_1(self.conv_upsample_1(x1_merge)))\n",
    "\n",
    "        #\n",
    "        out = self.relu(x_master + x_gpb)\n",
    "\n",
    "        return out\n",
    "\n",
    "class GAU(nn.Module):\n",
    "    def __init__(self, channels_high, channels_low, upsample=True):\n",
    "        super(GAU, self).__init__()\n",
    "        # Global Attention Upsample\n",
    "        self.upsample = upsample\n",
    "        self.conv3x3 = nn.Conv2d(channels_low, channels_low, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn_low = nn.BatchNorm2d(channels_low)\n",
    "\n",
    "        self.conv1x1 = nn.Conv2d(channels_high, channels_low, kernel_size=1, padding=0, bias=False)\n",
    "        self.bn_high = nn.BatchNorm2d(channels_low)\n",
    "\n",
    "        if upsample:\n",
    "            self.conv_upsample = nn.ConvTranspose2d(channels_high, channels_low, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "            self.bn_upsample = nn.BatchNorm2d(channels_low)\n",
    "        else:\n",
    "            self.conv_reduction = nn.Conv2d(channels_high, channels_low, kernel_size=1, padding=0, bias=False)\n",
    "            self.bn_reduction = nn.BatchNorm2d(channels_low)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, fms_high, fms_low, fm_mask=None):\n",
    "        \"\"\"\n",
    "        Use the high level features with abundant catagory information to weight the low level features with pixel\n",
    "        localization information. In the meantime, we further use mask feature maps with catagory-specific information\n",
    "        to localize the mask position.\n",
    "        :param fms_high: Features of high level. Tensor.\n",
    "        :param fms_low: Features of low level.  Tensor.\n",
    "        :param fm_mask:\n",
    "        :return: fms_att_upsample\n",
    "        \"\"\"\n",
    "        b, c, h, w = fms_high.shape\n",
    "\n",
    "        fms_high_gp = nn.AvgPool2d(fms_high.shape[2:])(fms_high).view(len(fms_high), c, 1, 1)\n",
    "        fms_high_gp = self.conv1x1(fms_high_gp)\n",
    "        fms_high_gp = self.bn_high(fms_high_gp)\n",
    "        fms_high_gp = self.relu(fms_high_gp)\n",
    "\n",
    "        # fms_low_mask = torch.cat([fms_low, fm_mask], dim=1)\n",
    "        fms_low_mask = self.conv3x3(fms_low)\n",
    "        fms_low_mask = self.bn_low(fms_low_mask)\n",
    "\n",
    "        fms_att = fms_low_mask * fms_high_gp\n",
    "        if self.upsample:\n",
    "            out = self.relu(\n",
    "                self.bn_upsample(self.conv_upsample(fms_high)) + fms_att)\n",
    "        else:\n",
    "            out = self.relu(\n",
    "                self.bn_reduction(self.conv_reduction(fms_high)) + fms_att)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class PAM_Module(nn.Module):\n",
    "    def __init__(self, in_dim):\n",
    "        super(PAM_Module, self).__init__()\n",
    "        self.channel_in = in_dim\n",
    "        self.query_conv = nn.Conv2d(in_dim, in_dim // 8, kernel_size=1, stride=1, padding='valid')\n",
    "        self.key_conv = nn.Conv2d(in_dim, in_dim // 8, kernel_size=1, stride=1, padding='valid')\n",
    "        self.value_conv = nn.Conv2d(in_dim, in_dim, kernel_size=1, stride=1, padding='valid')\n",
    "        self.gamma = nn.Parameter(torch.zeros(1))\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        m_batchsize, C, height, width = x.size()\n",
    "        proj_query = self.query_conv(x).view(m_batchsize, -1, width * height).permute(0, 2, 1)\n",
    "        proj_key = self.key_conv(x).view(m_batchsize, -1, width * height)\n",
    "        energy = torch.bmm(proj_query, proj_key)\n",
    "        attention = self.softmax(energy)\n",
    "        proj_value = self.value_conv(x).view(m_batchsize, -1, width * height)\n",
    "        proj_value = proj_value.permute(0, 2, 1)\n",
    "        out = torch.bmm(attention, proj_value).view(m_batchsize, C, height, width)\n",
    "        out = self.gamma * out + x\n",
    "        return out\n",
    "\n",
    "class CAM_Module(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CAM_Module, self).__init__()\n",
    "        self.gamma = nn.Parameter(torch.zeros(1))\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        m_batchsize, C, height, width = x.size()\n",
    "        proj_query = x.view(m_batchsize, C, -1)\n",
    "        proj_key = x.view(m_batchsize, C, -1).permute(0, 2, 1)\n",
    "        energy = torch.bmm(proj_query, proj_key)\n",
    "        energy_new = energy.max(dim=-1, keepdim=True)[0]\n",
    "        energy_new = energy_new - energy\n",
    "        attention = self.softmax(energy_new)\n",
    "        proj_value = x.view(m_batchsize, C, -1)\n",
    "\n",
    "        out = torch.bmm(attention, proj_value).view(m_batchsize, C, height, width)\n",
    "        out = self.gamma * out + x\n",
    "        return out\n",
    "    \n",
    "class PAN(nn.Module):\n",
    "    def __init__(self, blocks=[]):\n",
    "        \"\"\"\n",
    "        :param blocks: Blocks of the network with reverse sequential.\n",
    "        \"\"\"\n",
    "        super(PAN, self).__init__()\n",
    "        channels_blocks = []\n",
    "        for i, block in enumerate(blocks):\n",
    "            channels_blocks.append(list(list(block.children())[2].children())[4].weight.shape[0])\n",
    "\n",
    "        self.fpa = FPA(channels=channels_blocks[0])\n",
    "        # channels_high = channels_blocks[0]\n",
    "        # for i, channels_low in enumerate(channels_blocks[1:]):\n",
    "        #     self.gau.append(GAU(channels_high, channels_low))\n",
    "        #     channels_high = channels_low\n",
    "        self.gau_block1 = GAU(channels_blocks[0], channels_blocks[1], upsample=False)\n",
    "        self.gau_block2 = GAU(channels_blocks[1], channels_blocks[2])\n",
    "        self.gau_block3 = GAU(channels_blocks[2], channels_blocks[3])\n",
    "        self.gau = [self.gau_block1, self.gau_block2, self.gau_block3]\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, fms=[]):\n",
    "        \"\"\"\n",
    "        :param fms: Feature maps of forward propagation in the network with reverse sequential. shape:[b, c, h, w]\n",
    "        :return: fm_high. [b, 256, h, w]\n",
    "        \"\"\"\n",
    "        for i, fm_low in enumerate(fms):\n",
    "            if i == 0:\n",
    "                fm_high = self.fpa(fm_low)\n",
    "            else:\n",
    "                fm_high = self.gau[int(i-1)](fm_high, fm_low)\n",
    "\n",
    "        return fm_high\n",
    "\n",
    "class PAN_DA(nn.Module):\n",
    "    def __init__(self, in_channels=512, num_class=8):\n",
    "        super(PAN_DA, self).__init__()\n",
    "        \n",
    "        self.convnet = ResNet34(pretrained=True)\n",
    "        self.pan = PAN(self.convnet.blocks[::-1])\n",
    "        inter_channels = in_channels // 4\n",
    "        out_channels = in_channels // 4\n",
    "        self.conv5a = nn.Conv2d(in_channels, inter_channels, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(inter_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv5c = nn.Conv2d(in_channels, inter_channels, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(inter_channels)\n",
    "        self.sa = PAM_Module(inter_channels)\n",
    "        self.sc = CAM_Module()\n",
    "        self.conv51 = nn.Conv2d(inter_channels, inter_channels, kernel_size=3, padding=1)\n",
    "        self.conv52 = nn.Conv2d(inter_channels, inter_channels, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(inter_channels)\n",
    "        self.bn4 = nn.BatchNorm2d(inter_channels)\n",
    "        self.dropout = nn.Dropout2d(0.1)\n",
    "        self.conv6 = nn.Conv2d(inter_channels, out_channels, kernel_size=1)\n",
    "        self.conv7 = nn.Conv2d(inter_channels, out_channels, kernel_size=1)\n",
    "        self.conv8 = nn.Conv2d(inter_channels, out_channels, kernel_size=1)\n",
    "        \n",
    "        self.cls = Classifier(in_features=64, num_class=num_class)\n",
    "        self.avgpool1 = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.avgpool2 = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "        self.fc1 = Classifier(in_features=out_channels, num_class=num_class)\n",
    "        self.fcf = Classifier(in_features=2*num_class, num_class=num_class)\n",
    "        self.feat = None\n",
    "        self.gradients = None\n",
    "        \n",
    "    def activations_hook(self, grad):\n",
    "        self.gradients = grad\n",
    "        \n",
    "    def forward(self, x):\n",
    "        fms_blob, _ = self.convnet(x)\n",
    "        out_ss = self.pan(fms_blob[::-1])\n",
    "        self.feat = out_ss\n",
    "        h = out_ss.register_hook(self.activations_hook)\n",
    "        out1 = self.avgpool1(out_ss)\n",
    "        out1 = out1.view(out1.size(0), -1)\n",
    "        out1 = self.cls(out1)\n",
    "\n",
    "        feat1 = self.relu(self.bn1(self.conv5a(fms_blob[-1])))\n",
    "        sa_feat = self.sa(feat1)\n",
    "        sa_conv = self.relu(self.bn3(self.conv51(sa_feat)))\n",
    "        sa_output = self.conv6(self.dropout(sa_conv))\n",
    "\n",
    "        feat2 = self.relu(self.bn2(self.conv5c(fms_blob[-1])))\n",
    "        sc_feat = self.sc(feat2)\n",
    "        sc_conv = self.relu(self.bn4(self.conv52(sc_feat)))\n",
    "        sc_output = self.conv7(self.dropout(sc_conv))\n",
    "\n",
    "        feat_sum = sa_output + sc_output\n",
    "        sasc_output = self.conv8(self.dropout(feat_sum))\n",
    "        out2 = self.avgpool2(sasc_output)\n",
    "        out2 = out2.view(out2.shape[0], -1)\n",
    "        out2 = self.fc1(out2)\n",
    "        \n",
    "        out3 = self.fcf(torch.cat([out1, out2], dim=1))\n",
    "        \n",
    "        return out3\n",
    "    \n",
    "    def get_activations_gradient(self):\n",
    "        return self.gradients\n",
    "    \n",
    "    def get_activations(self):\n",
    "        return self.feat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a8da31",
   "metadata": {},
   "source": [
    "## PAN_Inverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586e8148",
   "metadata": {},
   "outputs": [],
   "source": [
    "import resnet\n",
    "\n",
    "class ResNet50(nn.Module):\n",
    "    def __init__(self, pretrained=True):\n",
    "        \"\"\"Declare all needed layers.\"\"\"\n",
    "        super(ResNet50, self).__init__()\n",
    "        self.model = resnet.resnet50(pretrained=pretrained)\n",
    "        self.relu = self.model.relu  # Place a hook\n",
    "\n",
    "        layers_cfg = [4, 5, 6, 7]\n",
    "        self.blocks = []\n",
    "        for i, num_this_layer in enumerate(layers_cfg):\n",
    "            self.blocks.append(list(self.model.children())[num_this_layer])\n",
    "\n",
    "    def forward(self, x):\n",
    "        feature_map = []\n",
    "        x = self.model.conv1(x)\n",
    "        x = self.model.bn1(x)\n",
    "        x = self.model.relu(x)\n",
    "        x = self.model.maxpool(x)\n",
    "\n",
    "        for i, block in enumerate(self.blocks):\n",
    "            x = block(x)\n",
    "            feature_map.append(x)\n",
    "\n",
    "        out = nn.AvgPool2d(x.shape[2:])(x).view(x.shape[0], -1)\n",
    "\n",
    "        return feature_map, out\n",
    "    \n",
    "class ResNet34(nn.Module):\n",
    "    def __init__(self, pretrained=True):\n",
    "        \"\"\"Declare all needed layers.\"\"\"\n",
    "        super(ResNet34, self).__init__()\n",
    "        self.model = resnet.resnet34(pretrained=pretrained)\n",
    "        self.relu = self.model.relu  # Place a hook\n",
    "\n",
    "        layers_cfg = [4, 5, 6, 7]\n",
    "        self.blocks = []\n",
    "        for i, num_this_layer in enumerate(layers_cfg):\n",
    "            self.blocks.append(list(self.model.children())[num_this_layer])\n",
    "\n",
    "    def forward(self, x):\n",
    "        feature_map = []\n",
    "        x = self.model.conv1(x)\n",
    "        x = self.model.bn1(x)\n",
    "        x = self.model.relu(x)\n",
    "        x = self.model.maxpool(x)\n",
    "\n",
    "        for i, block in enumerate(self.blocks):\n",
    "            x = block(x)\n",
    "            feature_map.append(x)\n",
    "\n",
    "        out = nn.AvgPool2d(x.shape[2:])(x).view(x.shape[0], -1)\n",
    "\n",
    "        return feature_map, out\n",
    "\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, in_features=2048, num_class=20):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_features, num_class)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "\n",
    "class FPA(nn.Module):\n",
    "    def __init__(self, channels=2048):\n",
    "        \"\"\"\n",
    "        Feature Pyramid Attention\n",
    "        :type channels: int\n",
    "        \"\"\"\n",
    "        super(FPA, self).__init__()\n",
    "        channels_mid = int(channels/4)\n",
    "\n",
    "        self.channels_cond = channels\n",
    "\n",
    "        # Master branch\n",
    "        self.conv_master = nn.Conv2d(self.channels_cond, channels, kernel_size=1, bias=False)\n",
    "        self.bn_master = nn.BatchNorm2d(channels)\n",
    "\n",
    "        # Global pooling branch\n",
    "        self.conv_gpb = nn.Conv2d(self.channels_cond, channels, kernel_size=1, bias=False)\n",
    "        self.bn_gpb = nn.BatchNorm2d(channels)\n",
    "\n",
    "        # C333 because of the shape of last feature maps is (16, 16).\n",
    "        self.conv7x7_1 = nn.Conv2d(self.channels_cond, channels_mid, kernel_size=(7, 7), stride=2, padding=3, bias=False)\n",
    "        self.bn1_1 = nn.BatchNorm2d(channels_mid)\n",
    "        self.conv5x5_1 = nn.Conv2d(channels_mid, channels_mid, kernel_size=(5, 5), stride=2, padding=2, bias=False)\n",
    "        self.bn2_1 = nn.BatchNorm2d(channels_mid)\n",
    "        self.conv3x3_1 = nn.Conv2d(channels_mid, channels_mid, kernel_size=(3, 3), stride=2, padding=1, bias=False)\n",
    "        self.bn3_1 = nn.BatchNorm2d(channels_mid)\n",
    "\n",
    "        self.conv7x7_2 = nn.Conv2d(channels_mid, channels_mid, kernel_size=(7, 7), stride=1, padding=3, bias=False)\n",
    "        self.bn1_2 = nn.BatchNorm2d(channels_mid)\n",
    "        self.conv5x5_2 = nn.Conv2d(channels_mid, channels_mid, kernel_size=(5, 5), stride=1, padding=2, bias=False)\n",
    "        self.bn2_2 = nn.BatchNorm2d(channels_mid)\n",
    "        self.conv3x3_2 = nn.Conv2d(channels_mid, channels_mid, kernel_size=(3, 3), stride=1, padding=1, bias=False)\n",
    "        self.bn3_2 = nn.BatchNorm2d(channels_mid)\n",
    "\n",
    "        # Convolution Upsample\n",
    "        self.conv_upsample_3 = nn.ConvTranspose2d(channels_mid, channels_mid, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "        self.bn_upsample_3 = nn.BatchNorm2d(channels_mid)\n",
    "\n",
    "        self.conv_upsample_2 = nn.ConvTranspose2d(channels_mid, channels_mid, kernel_size=4, stride=1, padding=0, bias=False)\n",
    "        self.bn_upsample_2 = nn.BatchNorm2d(channels_mid)\n",
    "\n",
    "        self.conv_upsample_1 = nn.ConvTranspose2d(channels_mid, channels, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "        self.bn_upsample_1 = nn.BatchNorm2d(channels)\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: Shape: [b, 2048, h, w]\n",
    "        :return: out: Feature maps. Shape: [b, 2048, h, w]\n",
    "        \"\"\"\n",
    "        # Master branch\n",
    "        x_master = self.conv_master(x)\n",
    "        x_master = self.bn_master(x_master)\n",
    "\n",
    "        # Global pooling branch\n",
    "        x_gpb = nn.AvgPool2d(x.shape[2:])(x).view(x.shape[0], self.channels_cond, 1, 1)\n",
    "        x_gpb = self.conv_gpb(x_gpb)\n",
    "        x_gpb = self.bn_gpb(x_gpb)\n",
    "\n",
    "        # Branch 1\n",
    "        x1_1 = self.conv7x7_1(x)\n",
    "        x1_1 = self.bn1_1(x1_1)\n",
    "        x1_1 = self.relu(x1_1)\n",
    "        x1_2 = self.conv7x7_2(x1_1)\n",
    "        x1_2 = self.bn1_2(x1_2)\n",
    "\n",
    "        # Branch 2\n",
    "        x2_1 = self.conv5x5_1(x1_1)\n",
    "        x2_1 = self.bn2_1(x2_1)\n",
    "        x2_1 = self.relu(x2_1)\n",
    "        x2_2 = self.conv5x5_2(x2_1)\n",
    "        x2_2 = self.bn2_2(x2_2)\n",
    "\n",
    "        # Branch 3\n",
    "        x3_1 = self.conv3x3_1(x2_1)\n",
    "        x3_1 = self.bn3_1(x3_1)\n",
    "        x3_1 = self.relu(x3_1)\n",
    "        x3_2 = self.conv3x3_2(x3_1)\n",
    "        x3_2 = self.bn3_2(x3_2)\n",
    "\n",
    "        # Merge branch 1 and 2\n",
    "        x3_upsample = self.relu(self.bn_upsample_3(self.conv_upsample_3(x3_2)))\n",
    "        x2_merge = self.relu(x2_2 + x3_upsample)\n",
    "        x2_upsample = self.relu(self.bn_upsample_2(self.conv_upsample_2(x2_merge)))\n",
    "        x1_merge = self.relu(x1_2 + x2_upsample)\n",
    "\n",
    "        x_master = x_master * self.relu(self.bn_upsample_1(self.conv_upsample_1(x1_merge)))\n",
    "\n",
    "        #\n",
    "        out = self.relu(x_master + x_gpb)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class GAD(nn.Module):\n",
    "    def __init__(self, channels_low, channels_high, downsample=True):\n",
    "        super(GAD, self).__init__()\n",
    "        # Global Attention Upsample\n",
    "        self.downsample = downsample\n",
    "        self.conv3x3 = nn.Conv2d(channels_high, channels_high, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn_low = nn.BatchNorm2d(channels_high)\n",
    "\n",
    "        self.conv1x1 = nn.Conv2d(channels_low, channels_high, kernel_size=1, padding=0, bias=False)\n",
    "        self.bn_high = nn.BatchNorm2d(channels_high)\n",
    "\n",
    "        if self.downsample:\n",
    "            self.conv_down = nn.Conv2d(channels_low, channels_high, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "            self.bn_down = nn.BatchNorm2d(channels_high)\n",
    "        else:\n",
    "            self.conv_down = nn.Conv2d(channels_low, channels_high, kernel_size=3, padding=1, bias=False)\n",
    "            self.bn_down = nn.BatchNorm2d(channels_high)\n",
    "\n",
    "        self.bn_final = nn.BatchNorm2d(channels_high)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, fms_low, fms_high, fm_mask=None):\n",
    "\n",
    "        b, c, h, w = fms_low.shape\n",
    "\n",
    "        fms_low_gp = nn.AvgPool2d(fms_low.shape[2:])(fms_low).view(len(fms_low), c, 1, 1)\n",
    "        fms_low_gp = self.conv1x1(fms_low_gp)\n",
    "        fms_low_gp = self.bn_low(fms_low_gp)\n",
    "        fms_low_gp = self.relu(fms_low_gp)\n",
    "\n",
    "        # fms_low_mask = torch.cat([fms_low, fm_mask], dim=1)\n",
    "        fms_high_mask = self.conv3x3(fms_high)\n",
    "        fms_high_mask = self.bn_high(fms_high_mask)\n",
    "\n",
    "        fms_att = fms_high_mask * fms_low_gp\n",
    "\n",
    "        out = self.relu(self.bn_final(fms_att + self.bn_down(self.conv_down(fms_low))))\n",
    "\n",
    "\n",
    "\n",
    "        return out\n",
    "    \n",
    "class PAN(nn.Module):\n",
    "    def __init__(self, blocks=[]):\n",
    "\n",
    "        super(PAN, self).__init__()\n",
    "        channels_blocks = []\n",
    "        for i, block in enumerate(blocks):\n",
    "            channels_blocks.append(list(list(block.children())[2].children())[4].weight.shape[0])\n",
    "\n",
    "        self.fpa = FPA(channels=channels_blocks[-1])\n",
    "        # channels_high = channels_blocks[0]\n",
    "        # for i, channels_low in enumerate(channels_blocks[1:]):\n",
    "        #     self.gau.append(GAU(channels_high, channels_low))\n",
    "        #     channels_high = channels_low\n",
    "        self.gau_block1 = GAD(channels_blocks[0], channels_blocks[1])\n",
    "        self.gau_block2 = GAD(channels_blocks[1], channels_blocks[2])\n",
    "        self.gau_block3 = GAD(channels_blocks[2], channels_blocks[3], downsample=False)\n",
    "        self.gau = [self.gau_block1, self.gau_block2, self.gau_block3]\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, fms=[]):\n",
    "\n",
    "        for i, fm_low in enumerate(fms):\n",
    "            if i == len(fms) - 1:\n",
    "                fm_high = self.gau[int(i-1)](fm_high, fm_low)\n",
    "                fm_high = self.fpa(fm_high)\n",
    "            elif i == 0:\n",
    "                fm_high = fm_low\n",
    "            else:\n",
    "                fm_high = self.gau[int(i-1)](fm_high, fm_low)\n",
    "\n",
    "        return fm_high\n",
    "\n",
    "class PAN_Inverse(nn.Module):\n",
    "    def __init__(self, num_class=8):\n",
    "        super(PAN_Inverse, self).__init__()\n",
    "        \n",
    "        self.convnet = ResNet34(pretrained=True)\n",
    "        self.pan = PAN(self.convnet.blocks)\n",
    "        self.cls = Classifier(in_features=512, num_class=num_class)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.feat = None\n",
    "        self.gradients = None\n",
    "        \n",
    "    def activations_hook(self, grad):\n",
    "        self.gradients = grad\n",
    "        \n",
    "    def forward(self, x):\n",
    "        fms_blob, _ = self.convnet(x)\n",
    "        out_ss = self.pan(fms_blob)\n",
    "        self.feat = out_ss\n",
    "        h = out_ss.register_hook(self.activations_hook)\n",
    "        out = self.avgpool(out_ss)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.cls(out)\n",
    "        return out\n",
    "    \n",
    "    def get_activations_gradient(self):\n",
    "        return self.gradients\n",
    "    \n",
    "    def get_activations(self):\n",
    "        return self.feat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eecda6b6",
   "metadata": {},
   "source": [
    "## PAN_DualInverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b719d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import resnet\n",
    "\n",
    "class ResNet50(nn.Module):\n",
    "    def __init__(self, pretrained=True):\n",
    "        \"\"\"Declare all needed layers.\"\"\"\n",
    "        super(ResNet50, self).__init__()\n",
    "        self.model = resnet.resnet50(pretrained=pretrained)\n",
    "        self.relu = self.model.relu  # Place a hook\n",
    "\n",
    "        layers_cfg = [4, 5, 6, 7]\n",
    "        self.blocks = []\n",
    "        for i, num_this_layer in enumerate(layers_cfg):\n",
    "            self.blocks.append(list(self.model.children())[num_this_layer])\n",
    "\n",
    "    def forward(self, x):\n",
    "        feature_map = []\n",
    "        x = self.model.conv1(x)\n",
    "        x = self.model.bn1(x)\n",
    "        x = self.model.relu(x)\n",
    "        x = self.model.maxpool(x)\n",
    "\n",
    "        for i, block in enumerate(self.blocks):\n",
    "            x = block(x)\n",
    "            feature_map.append(x)\n",
    "\n",
    "        out = nn.AvgPool2d(x.shape[2:])(x).view(x.shape[0], -1)\n",
    "\n",
    "        return feature_map, out\n",
    "    \n",
    "class ResNet34(nn.Module):\n",
    "    def __init__(self, pretrained=True):\n",
    "        \"\"\"Declare all needed layers.\"\"\"\n",
    "        super(ResNet34, self).__init__()\n",
    "        self.model = resnet.resnet34(pretrained=pretrained)\n",
    "        self.relu = self.model.relu  # Place a hook\n",
    "\n",
    "        layers_cfg = [4, 5, 6, 7]\n",
    "        self.blocks = []\n",
    "        for i, num_this_layer in enumerate(layers_cfg):\n",
    "            self.blocks.append(list(self.model.children())[num_this_layer])\n",
    "\n",
    "    def forward(self, x):\n",
    "        feature_map = []\n",
    "        x = self.model.conv1(x)\n",
    "        x = self.model.bn1(x)\n",
    "        x = self.model.relu(x)\n",
    "        x = self.model.maxpool(x)\n",
    "\n",
    "        for i, block in enumerate(self.blocks):\n",
    "            x = block(x)\n",
    "            feature_map.append(x)\n",
    "\n",
    "        out = nn.AvgPool2d(x.shape[2:])(x).view(x.shape[0], -1)\n",
    "\n",
    "        return feature_map, out\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, in_features=2048, num_class=20):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_features, num_class)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "\n",
    "class FPA(nn.Module):\n",
    "    def __init__(self, channels=2048):\n",
    "        \"\"\"\n",
    "        Feature Pyramid Attention\n",
    "        :type channels: int\n",
    "        \"\"\"\n",
    "        super(FPA, self).__init__()\n",
    "        channels_mid = int(channels/4)\n",
    "\n",
    "        self.channels_cond = channels\n",
    "\n",
    "        # Master branch\n",
    "        self.conv_master = nn.Conv2d(self.channels_cond, channels, kernel_size=1, bias=False)\n",
    "        self.bn_master = nn.BatchNorm2d(channels)\n",
    "\n",
    "        # Global pooling branch\n",
    "        self.conv_gpb = nn.Conv2d(self.channels_cond, channels, kernel_size=1, bias=False)\n",
    "        self.bn_gpb = nn.BatchNorm2d(channels)\n",
    "\n",
    "        # C333 because of the shape of last feature maps is (16, 16).\n",
    "        self.conv7x7_1 = nn.Conv2d(self.channels_cond, channels_mid, kernel_size=(7, 7), stride=2, padding=3, bias=False)\n",
    "        self.bn1_1 = nn.BatchNorm2d(channels_mid)\n",
    "        self.conv5x5_1 = nn.Conv2d(channels_mid, channels_mid, kernel_size=(5, 5), stride=2, padding=2, bias=False)\n",
    "        self.bn2_1 = nn.BatchNorm2d(channels_mid)\n",
    "        self.conv3x3_1 = nn.Conv2d(channels_mid, channels_mid, kernel_size=(3, 3), stride=2, padding=1, bias=False)\n",
    "        self.bn3_1 = nn.BatchNorm2d(channels_mid)\n",
    "\n",
    "        self.conv7x7_2 = nn.Conv2d(channels_mid, channels_mid, kernel_size=(7, 7), stride=1, padding=3, bias=False)\n",
    "        self.bn1_2 = nn.BatchNorm2d(channels_mid)\n",
    "        self.conv5x5_2 = nn.Conv2d(channels_mid, channels_mid, kernel_size=(5, 5), stride=1, padding=2, bias=False)\n",
    "        self.bn2_2 = nn.BatchNorm2d(channels_mid)\n",
    "        self.conv3x3_2 = nn.Conv2d(channels_mid, channels_mid, kernel_size=(3, 3), stride=1, padding=1, bias=False)\n",
    "        self.bn3_2 = nn.BatchNorm2d(channels_mid)\n",
    "\n",
    "        # Convolution Upsample\n",
    "        self.conv_upsample_3 = nn.ConvTranspose2d(channels_mid, channels_mid, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "        self.bn_upsample_3 = nn.BatchNorm2d(channels_mid)\n",
    "\n",
    "        self.conv_upsample_2 = nn.ConvTranspose2d(channels_mid, channels_mid, kernel_size=4, stride=1, padding=0, bias=False)\n",
    "        self.bn_upsample_2 = nn.BatchNorm2d(channels_mid)\n",
    "\n",
    "        self.conv_upsample_1 = nn.ConvTranspose2d(channels_mid, channels, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "        self.bn_upsample_1 = nn.BatchNorm2d(channels)\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: Shape: [b, 2048, h, w]\n",
    "        :return: out: Feature maps. Shape: [b, 2048, h, w]\n",
    "        \"\"\"\n",
    "        # Master branch\n",
    "        x_master = self.conv_master(x)\n",
    "        x_master = self.bn_master(x_master)\n",
    "\n",
    "        # Global pooling branch\n",
    "        x_gpb = nn.AvgPool2d(x.shape[2:])(x).view(x.shape[0], self.channels_cond, 1, 1)\n",
    "        x_gpb = self.conv_gpb(x_gpb)\n",
    "        x_gpb = self.bn_gpb(x_gpb)\n",
    "\n",
    "        # Branch 1\n",
    "        x1_1 = self.conv7x7_1(x)\n",
    "        x1_1 = self.bn1_1(x1_1)\n",
    "        x1_1 = self.relu(x1_1)\n",
    "        x1_2 = self.conv7x7_2(x1_1)\n",
    "        x1_2 = self.bn1_2(x1_2)\n",
    "\n",
    "        # Branch 2\n",
    "        x2_1 = self.conv5x5_1(x1_1)\n",
    "        x2_1 = self.bn2_1(x2_1)\n",
    "        x2_1 = self.relu(x2_1)\n",
    "        x2_2 = self.conv5x5_2(x2_1)\n",
    "        x2_2 = self.bn2_2(x2_2)\n",
    "\n",
    "        # Branch 3\n",
    "        x3_1 = self.conv3x3_1(x2_1)\n",
    "        x3_1 = self.bn3_1(x3_1)\n",
    "        x3_1 = self.relu(x3_1)\n",
    "        x3_2 = self.conv3x3_2(x3_1)\n",
    "        x3_2 = self.bn3_2(x3_2)\n",
    "\n",
    "        # Merge branch 1 and 2\n",
    "        x3_upsample = self.relu(self.bn_upsample_3(self.conv_upsample_3(x3_2)))\n",
    "        x2_merge = self.relu(x2_2 + x3_upsample)\n",
    "        x2_upsample = self.relu(self.bn_upsample_2(self.conv_upsample_2(x2_merge)))\n",
    "        x1_merge = self.relu(x1_2 + x2_upsample)\n",
    "\n",
    "        x_master = x_master * self.relu(self.bn_upsample_1(self.conv_upsample_1(x1_merge)))\n",
    "\n",
    "        #\n",
    "        out = self.relu(x_master + x_gpb)\n",
    "\n",
    "        return out\n",
    "\n",
    "class GAD(nn.Module):\n",
    "    def __init__(self, channels_low, channels_high, downsample=True):\n",
    "        super(GAD, self).__init__()\n",
    "        # Global Attention Upsample\n",
    "        self.downsample = downsample\n",
    "        self.conv3x3 = nn.Conv2d(channels_high, channels_high, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn_low = nn.BatchNorm2d(channels_high)\n",
    "\n",
    "        self.conv1x1 = nn.Conv2d(channels_low, channels_high, kernel_size=1, padding=0, bias=False)\n",
    "        self.bn_high = nn.BatchNorm2d(channels_high)\n",
    "\n",
    "        if self.downsample:\n",
    "            self.conv_down = nn.Conv2d(channels_low, channels_high, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "            self.bn_down = nn.BatchNorm2d(channels_high)\n",
    "        else:\n",
    "            self.conv_down = nn.Conv2d(channels_low, channels_high, kernel_size=3, padding=1, bias=False)\n",
    "            self.bn_down = nn.BatchNorm2d(channels_high)\n",
    "\n",
    "        self.bn_final = nn.BatchNorm2d(channels_high)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, fms_low, fms_high, fm_mask=None):\n",
    "\n",
    "        b, c, h, w = fms_low.shape\n",
    "\n",
    "        fms_low_gp = nn.AvgPool2d(fms_low.shape[2:])(fms_low).view(len(fms_low), c, 1, 1)\n",
    "        fms_low_gp = self.conv1x1(fms_low_gp)\n",
    "        fms_low_gp = self.bn_low(fms_low_gp)\n",
    "        fms_low_gp = self.relu(fms_low_gp)\n",
    "\n",
    "        # fms_low_mask = torch.cat([fms_low, fm_mask], dim=1)\n",
    "        fms_high_mask = self.conv3x3(fms_high)\n",
    "        fms_high_mask = self.bn_high(fms_high_mask)\n",
    "\n",
    "        fms_att = fms_high_mask * fms_low_gp\n",
    "\n",
    "        out = self.relu(self.bn_final(fms_att + self.bn_down(self.conv_down(fms_low))))\n",
    "\n",
    "\n",
    "\n",
    "        return out\n",
    "\n",
    "class GADS(nn.Module):\n",
    "    def __init__(self, in_channels, hw_low, hw_high):\n",
    "        super(GADS, self).__init__()\n",
    "        self.conv3x3 = nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1)\n",
    "        self.bn_low = nn.BatchNorm2d(in_channels)\n",
    "        \n",
    "        \n",
    "        \n",
    "        if hw_low // hw_high == 2:\n",
    "            self.conv1 = nn.Conv2d(2, 1, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "            self.conv_down = nn.Conv2d(in_channels//2, in_channels, kernel_size=3, stride=2, padding=2, dilation=2)\n",
    "            self.bn_down = nn.BatchNorm2d(in_channels)\n",
    "        elif hw_low == hw_high:\n",
    "            self.conv1 = nn.Conv2d(2, 1, kernel_size=7, padding=3, bias=False)\n",
    "            self.conv_down = nn.Conv2d(in_channels//2, in_channels, kernel_size=3, padding=1)\n",
    "            self.bn_down = nn.BatchNorm2d(in_channels)\n",
    "            \n",
    "        self.bn_final = nn.BatchNorm2d(in_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, fms_low, fms_high, fm_mask=None):\n",
    "        \n",
    "        avg_out = torch.mean(fms_low, dim=1, keepdim=True)\n",
    "        max_out, _ = torch.max(fms_low, dim=1, keepdim=True)\n",
    "        mask_x = torch.cat([avg_out, max_out], dim=1)\n",
    "        mask_x = self.conv1(mask_x)\n",
    "        \n",
    "        x = self.conv3x3(fms_high)\n",
    "        x = self.bn_low(x)\n",
    "\n",
    "        fms_att = x * mask_x\n",
    "        out = self.relu(fms_att + self.bn_down(self.conv_down(fms_low)))\n",
    "\n",
    "        return out\n",
    "    \n",
    "class PAN(nn.Module):\n",
    "    def __init__(self, blocks=[]):\n",
    "\n",
    "        super(PAN, self).__init__()\n",
    "        channels_blocks = []\n",
    "        spatial_blocks = [56, 28, 14, 14]\n",
    "        for i, block in enumerate(blocks):\n",
    "            channels_blocks.append(list(list(block.children())[2].children())[4].weight.shape[0])\n",
    "            \n",
    "        self.fpa = FPA(channels=channels_blocks[-1])\n",
    "        self.fpas = FPA(channels=channels_blocks[-1])\n",
    "\n",
    "        self.gad_block1 = GAD(channels_blocks[0], channels_blocks[1])\n",
    "        self.gad_block2 = GAD(channels_blocks[1], channels_blocks[2])\n",
    "        self.gad_block3 = GAD(channels_blocks[2], channels_blocks[3], downsample=False)\n",
    "        self.gad = [self.gad_block1, self.gad_block2, self.gad_block3]\n",
    "        \n",
    "        self.gads_block1 = GADS(channels_blocks[1], spatial_blocks[0], spatial_blocks[1])\n",
    "        self.gads_block2 = GADS(channels_blocks[2], spatial_blocks[1], spatial_blocks[2])\n",
    "        self.gads_block3 = GADS(channels_blocks[3], spatial_blocks[2], spatial_blocks[3])\n",
    "        self.gads = [self.gads_block1, self.gads_block2, self.gads_block3]\n",
    "        \n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, fms=[]):\n",
    "\n",
    "        for i, fm_low in enumerate(fms):\n",
    "            if i == len(fms) - 1:\n",
    "                fm1 = self.gad[int(i-1)](fm_high, fm_low)\n",
    "                fm1 = self.fpa(fm1)\n",
    "            elif i == 0:\n",
    "                fm_high = fm_low\n",
    "            else:\n",
    "                fm_high = self.gad[int(i-1)](fm_high, fm_low)\n",
    "                \n",
    "        for i, fm_low in enumerate(fms):\n",
    "            if i == len(fms) - 1:\n",
    "                fm2 = self.gads[int(i-1)](fm_high, fm_low)\n",
    "                fm2 = self.fpas(fm2)\n",
    "            elif i == 0:\n",
    "                fm_high = fm_low\n",
    "            else:\n",
    "                fm_high = self.gads[int(i-1)](fm_high, fm_low)\n",
    "                       \n",
    "        out = self.relu(fm1 + fm2)\n",
    "        return out\n",
    "\n",
    "class PAN_DualInverse(nn.Module):\n",
    "    def __init__(self, num_class=8):\n",
    "        super(PAN_DualInverse, self).__init__()\n",
    "        \n",
    "        self.convnet = ResNet34(pretrained=True)\n",
    "        self.pan = PAN(self.convnet.blocks)\n",
    "        self.cls = Classifier(in_features=512, num_class=num_class)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.feat = None\n",
    "        self.gradients = None\n",
    "        \n",
    "    def activations_hook(self, grad):\n",
    "        self.gradients = grad\n",
    "        \n",
    "    def forward(self, x):\n",
    "        fms_blob, _ = self.convnet(x)\n",
    "        out_ss = self.pan(fms_blob)\n",
    "        self.feat = out_ss\n",
    "        h = out_ss.register_hook(self.activations_hook)\n",
    "        out = self.avgpool(out_ss)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.cls(out)\n",
    "        return out\n",
    "    \n",
    "    def get_activations_gradient(self):\n",
    "        return self.gradients\n",
    "    \n",
    "    def get_activations(self):\n",
    "        return self.feat"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
